{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Business - Convolutional Neural Network (CNN) on Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize notebook\n",
    "Load required packages and set up workspace, e.g., initialize the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from torchmetrics import Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1688e3690>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "The Fashion MNIST dataset is a dataset of thumbnails of fashion articles from Zalando (28x28 grayscale images). The dataset consists of a training set of 60,000 examples and a test set of 10,000 examples. Each image is associated with a label from 10 classes (e.g., Shirts, Sneakers, Trousers...). The dataset is balanced in terms of label distribution.\n",
    "\n",
    "You can have a look at the dataset here: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Your task is to train a CNN classifier to predict the class of the fashion articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The dataset is built into the Pytorch library, so we can load it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a number of transformations during loading. For now, we only convert the images to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dimensions of the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.data.shape)\n",
    "print(test_loader.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first image in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "           0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,   1,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,   0,\n",
       "          36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,   0,   3],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,\n",
       "         102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,  10,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,  72,  15],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,  69,\n",
       "         207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88, 172,  66],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0, 200,\n",
       "         232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196, 229,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 183,\n",
       "         225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245, 173,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 193,\n",
       "         228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243, 202,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12, 219,\n",
       "         220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197, 209,  52],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99, 244,\n",
       "         222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119, 167,  56],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55, 236,\n",
       "         228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,  92,   0],\n",
       "        [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237, 226,\n",
       "         217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,  77,   0],\n",
       "        [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228, 207,\n",
       "         213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244, 159,   0],\n",
       "        [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217, 226,\n",
       "         200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238, 215,   0],\n",
       "        [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200, 159,\n",
       "         245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232, 246,   0],\n",
       "        [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,  80,\n",
       "         150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228, 225,   0],\n",
       "        [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217, 241,\n",
       "          65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224, 229,  29],\n",
       "        [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198, 213,\n",
       "         240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221, 230,  67],\n",
       "        [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219, 221,\n",
       "         220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205, 206, 115],\n",
       "        [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211, 210,\n",
       "         200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177, 210,  92],\n",
       "        [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189, 188,\n",
       "         193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216, 170,   0],\n",
       "        [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244, 221,\n",
       "         220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this tensor of integers and pass it to the `imshow()` function of `matplotlib` to visualize the image. Looks like a sneaker! From Nike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAON0lEQVR4nO3cy2/U9RrH8Wfa0lKYAnJLQYnVCJGNwYiK10QjRncGE9wawsa9/4ELja7duXStC+Mt7sEoMQYWbARvgOEmNEDvnTm7Jyc5J+k836S1Oef1WvvhN5lOeTMLn06/3+8HAETE0D/9AgBYP0QBgCQKACRRACCJAgBJFABIogBAEgUA0sig/2Gn01nN1wHAKhvk/1X2TQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDSyD/9AmAlnU6nvOn3+6vwSv7TxMREefP88883Pevrr79u2lW1vN/Dw8PlzdLSUnmz3rW8d61W6zPumwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJKDeKx7Q0P1f7ssLy+XN4888kh5c/LkyfJmdna2vImIuHfvXnkzNzdX3vzwww/lzVoet2s5OtfyGWp5zlq+Dy1HCAfhmwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJKDeKx7LYe/Wg7ivfzyy+XNK6+8Ut5cunSpvImIGBsbK282bdpU3hw9erS8+eSTT8qbq1evljcREf1+v7xp+Ty06Ha7Tbter1fezMzMND1rJb4pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgOYjHurewsLAmz3nyySfLm6mpqfKm5cBfRMTQUP3fcN9++2158/jjj5c3H374YXlz5syZ8iYi4ty5c+XN+fPny5unnnqqvGn5DEVEnDp1qrw5ffp007NW4psCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSg3ismU6n07Tr9/vlzdGjR8ubw4cPlzd37twpbzZv3lzeREQcOHBgTTY//vhjefPLL7+UN91ut7yJiHjmmWfKm2PHjpU3i4uL5U3LexcRcfLkyfJmfn6+6Vkr8U0BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABInf6AJyhbL1yy/q33n23LldTvv/++vJmamipvWrS+30tLS+XNwsJC07Oq5ubmypter9f0rJ9++qm8abni2vJ+v/baa+VNRMTDDz9c3tx///3lzSC/S74pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgjfzTL4B/XsvBufXu1q1b5c2ePXvKm9nZ2fJmbGysvImIGBmp/7p2u93ypuW43fj4eHnTehDvhRdeKG+effbZ8mZoqP5v5t27d5c3ERHffPNN0241+KYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDkIB7/kzZt2lTetBxAa9nMzMyUNxER09PT5c3NmzfLm6mpqfKm5ahip9MpbyLa3vOWz8Py8nJ503rkb9++fU271eCbAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkoN4NB0mazlK1nJgLCKi2+2WN3v37i1v5ufn12QzNjZW3kRELCwslDctx/e2bdtW3rQc3ms5UhcRMTo6Wt7cuXOnvNm6dWt5c/bs2fImou0zfvjw4aZnrcQ3BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAILmSSvT7/fJmeHi4vGm9kvrWW2+VN5OTk+XN9evXy5vx8fHyptfrlTcREZs3by5v9u3bV960XGNtufy6uLhY3kREjIzU/9pq+Tnt2LGjvPn444/Lm4iIQ4cOlTct78MgfFMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDq9Ae8htbpdFb7tfAPaTmstbS0tAqv5L97+umny5svv/yyvJmdnS1v1vIw4MTERHkzNzdX3ty8ebO82bBhw5psItoOA966davpWVUt73dExEcffVTefPrpp+XNIH/d+6YAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUv4S2yloP77UcJhsaqjex5fUtLi6WN71er7xptZbH7Vp89dVX5c29e/fKm5aDeKOjo+XNgDco/8P169fLm5bfi40bN5Y3LZ/xVmv1+9Ty3j322GPlTUTE9PR00241+KYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYC0qgfxWg5KLS8vNz1rvR91W89efPHF8ubNN98sb5577rnyJiJiZmamvLl582Z503LcbmSk/ivU+hlveR9afgfHxsbKm5Yjeq2HAVvehxYtn4e7d+82PevYsWPlzRdffNH0rJX4pgBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgNTpD3iVqtPprPZrWXPbt28vb/bu3Vve7N+/f02eE9F2WOvAgQPlzfz8fHkzNNT2b5DFxcXyZnx8vLy5cuVKebNhw4bypuXQWkTEjh07ypuFhYXyZtOmTeXNqVOnyptut1veRLQdcOz1euXN9PR0edPyeYiIuHr1anlz8ODB8maQv+59UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFANKqXkk9cuRIefPee++VNxERu3btKm+2bdtW3iwvL5c3w8PD5c3t27fLm4iIpaWl8qblKmbL9c3WS7uzs7Plzfnz58ub48ePlzdnzpwpbyYmJsqbiIj77ruvvJmammp6VtXFixfLm9b34c6dO+XNzMxMedNyabf18uuWLVvKm5bfW1dSASgRBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGANPBBvJGRkfIffvr06fJmz5495U1E26G6lk3LYa0WLUf0ItqOx62VrVu3Nu127txZ3rz99tvlzauvvlrevPPOO+XNlStXypuIiLm5ufLm119/LW9ajtvt37+/vNmxY0d5E9F2jHHDhg3lTcvBvpbnRET0er3y5sEHHyxvHMQDoEQUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDSwAfxTpw4Uf7DP/jgg/LmwoUL5U1ERLfbXZPN2NhYedOi9bBWy9G5P//8s7xpOeq2a9eu8iYiYmio/m+XycnJ8uaNN94obzZu3FjeTE1NlTcRbZ/XJ554Yk02LT+jlsN2rc8aHR1telZVp9Np2rX8vh85cqS8+eOPP1b8b3xTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAGhn0P7x27Vr5D285tDYxMVHeRETMz8+XNy2vr+UoWcsxri1btpQ3ERF///13efP777+XNy3vw+zsbHkTETE3N1feLC0tlTeff/55eXPu3LnypvUg3vbt28ublqNzt2/fLm8WFxfLm5afUUREr9crb1oOzrU8p/UgXsvfEQcOHGh61kp8UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBr4IN7ly5fLf3i/3y9vLl26VN5ERGzevLm82blzZ3nTcizsxo0b5c3169fLm4iIkZGBf6RpbGysvGk5MLZx48byJqLtSOLQUP3fOy0/p4MHD5Y39+7dK28i2g443rp1q7xp+Ty0vHctR/Qi2g7ptTxrfHy8vJmcnCxvIiKmp6fLm0OHDjU9ayW+KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAGngk5o///xz+Q//7LPPypsTJ06UNxERV65cKW8uXrxY3szNzZU33W63vGm5QhrRdtlxdHS0vBkeHi5v5ufny5uIiOXl5fKm5ULvzMxMefPXX3+VNy2vLaLtfWi5mrtWn/GFhYXyJqLtUnHLpuWyassF14iIhx56qLy5evVq07NW4psCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSpz/gda5Op7ParyUiIl5//fWm3bvvvlve7N69u7y5ceNGedNyjKvl+FlE26G6loN4LYfWWl5bRNtnr+XoXMsRwpZNy/vd+qy1+r1tec5qHXT7b1re816vV95MTk6WNxERZ8+eLW+OHz9e3gzye+GbAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUA0sAH8VqOmbUclFpLL730Unnz/vvvlzcth/e2bt1a3kREDA3VO9/ys205iNd65K/FtWvXypuWI3qXL18ub1p/L+7evVvetB4hrGp57xYXF5ueNTMzU960/F5899135c358+fLm4iIU6dONe2qHMQDoEQUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDSwAfxOp3Oar8W/s2jjz7atNu5c2d5c/v27fLmgQceKG9+++238iai7XDahQsXmp4F/8scxAOgRBQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBcSQX4P+FKKgAlogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABII4P+h/1+fzVfBwDrgG8KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKR/AcZJ2uKjdA3lAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_loader.dataset.data[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the label of this image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(9)\n",
      "List of labels: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "print(\"Label:\", train_loader.dataset.targets[0])\n",
    "print(\"List of labels:\", train_loader.dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with MLPs, we define the architecture of a CNN by creating a class that inherits form the `torch.nn.Module`.\n",
    "\n",
    "Our simple CNN has the following architecture:\n",
    "- Convolutional layer with 16 filters of size 3x3 (kernel) with a stride of 1 and padding of 1.\n",
    "- ReLU activation function.\n",
    "- Max pooling layer with a kernel size of 2x2 and stride of 2.\n",
    "- A layer without parameters that flattens the input.\n",
    "- Fully connected layer with 128 units.\n",
    "- ReLU activation function.\n",
    "- Fully connected layer with 10 units (one for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 128) # 16 out_channels of the convolution, 14x14 image dimension after pooling with kernel_size=2\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the CNN class and specify an optimizer and loss function. `CrossEntropyLoss` is a common loss function for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Linear-4                  [-1, 128]         401,536\n",
      "              ReLU-5                  [-1, 128]               0\n",
      "            Linear-6                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 402,986\n",
      "Trainable params: 402,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.22\n",
      "Params size (MB): 1.54\n",
      "Estimated Total Size (MB): 1.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is similar to the one for MLPs. We iterate over the training set, make predictions, calculate the loss, and update the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4753\n",
      "Epoch [2/5], Loss: 0.3075\n",
      "Epoch [3/5], Loss: 0.2641\n",
      "Epoch [4/5], Loss: 0.2348\n",
      "Epoch [5/5], Loss: 0.2128\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make predictions on the test set and calculate the accuracy of the model. Don't forget to set the model to evaluation mode before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "accuracy_metric.reset()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
    "        accuracy_metric.update(probabilities, labels)\n",
    "\n",
    "test_accuracy = accuracy_metric.compute()\n",
    "print(f\"Test Accuracy: {test_accuracy.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute predictions for an individual image. Let's take the first image of the training set (the Nike sneaker) and see what the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted label: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "logits = model(train_loader.dataset.data[0].unsqueeze(0).unsqueeze(0).float().to(device))\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "pred = torch.argmax(probs, dim=1)\n",
    "pred_label = train_loader.dataset.classes[pred]\n",
    "print(probs)\n",
    "print(f\"Predicted label: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
